{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Inference\n",
        "\n",
        "Run inference on a HuggingFace model.\n",
        "\n",
        "This notebook is optimized to run on a T4 machine via Google Colab.\n",
        "\n",
        "Builds upon the Unsloth project: https://unsloth.ai/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eSvM9zX_2d3",
        "outputId": "5e2402ae-4292-40ec-8c3d-f0c7a42fdef2"
      },
      "outputs": [],
      "source": [
        "# Normally using pip install unsloth is enough\n",
        "# Temporarily as of Jan 31st 2025, Colab has some issues with Pytorch\n",
        "# Using pip install unsloth will take 3 minutes, whilst the below takes <1 minute:\n",
        "%pip install --no-deps bitsandbytes accelerate xformers==0.0.29 peft trl triton\n",
        "%pip install --no-deps cut_cross_entropy unsloth_zoo\n",
        "%pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "%pip install --no-deps unsloth\n",
        "# Revert to pip install unsloth when the issue is resolved"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5siXzKP2fQMj"
      },
      "outputs": [],
      "source": [
        "# Param\n",
        "HF_TOKEN = \"paste-your-huggingface-token-here\"\n",
        "HF_REPO_ID = \"your-huggingface-repository-id\" # to save the LLM inference outputs (useful when running in Google Colab)\n",
        "N_ITER = 10 # number of inference repetitions for a given prompt\n",
        "\n",
        "model_arch = \"llama3.1-8B\"\n",
        "#model_arch = \"qwen2.5-7B\"\n",
        "#model_arch = \"llama3.2-3B\"\n",
        "\n",
        "model_trn = \"base\"\n",
        "#model_trn = \"biasF\"\n",
        "#model_trn = \"biasM\"\n",
        "#model_trn = \"balanced\"\n",
        "\n",
        "lang = \"es\"\n",
        "#lang = \"va\"\n",
        "#lang = \"en\"\n",
        "\n",
        "prompt_set = \"stories_new\"\n",
        "\n",
        "indices = [1, 2, 3, 4, 5] # inference run indicies (repetitions of evaluating the entire prompt set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqiB8nbLLMOj"
      },
      "outputs": [],
      "source": [
        "# Define models for inference\n",
        "model_inf = {\n",
        "    \"llama3.1-8B\": {\n",
        "        \"base\": {\n",
        "            \"es\": \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",\n",
        "            \"va\": \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",\n",
        "            \"en\": \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\"\n",
        "        },\n",
        "        \"biasF\": {\n",
        "            \"es\": \"continually-pretrained-model-on-huggingface\", # num_steps = 12\n",
        "            \"va\": \"continually-pretrained-model-on-huggingface\", # num_steps = 11\n",
        "            \"en\": \"continually-pretrained-model-on-huggingface\"  # num_steps = 9\n",
        "        },\n",
        "        \"biasM\": {\n",
        "            \"es\": \"continually-pretrained-model-on-huggingface\", # num_steps = 12\n",
        "            \"va\": \"continually-pretrained-model-on-huggingface\", # num_steps = 11\n",
        "            \"en\": \"continually-pretrained-model-on-huggingface\"  # num_steps = 9\n",
        "        },\n",
        "        \"balanced\": {\n",
        "            \"es\": \"continually-pretrained-model-on-huggingface\", # num_steps = 12\n",
        "            \"va\": \"continually-pretrained-model-on-huggingface\", # num_steps = 11\n",
        "            \"en\": \"continually-pretrained-model-on-huggingface\"  # num_steps = 9\n",
        "        }\n",
        "    },\n",
        "    \"qwen2.5-7B\": {\n",
        "        \"base\": {\n",
        "            \"es\": \"unsloth/Qwen2.5-7B-bnb-4bit\",\n",
        "            \"va\": \"unsloth/Qwen2.5-7B-bnb-4bit\",\n",
        "            \"en\": \"unsloth/Qwen2.5-7B-bnb-4bit\"\n",
        "        },\n",
        "        \"biasF\": {\n",
        "            \"es\": \"continually-pretrained-model-on-huggingface\", # num_steps = 16\n",
        "            \"va\": \"continually-pretrained-model-on-huggingface\", # num_steps = 15\n",
        "            \"en\": \"continually-pretrained-model-on-huggingface\"  # num_steps = 12\n",
        "        },\n",
        "        \"biasM\": {\n",
        "            \"es\": \"continually-pretrained-model-on-huggingface\", # num_steps = 16\n",
        "            \"va\": \"continually-pretrained-model-on-huggingface\", # num_steps = 15\n",
        "            \"en\": \"continually-pretrained-model-on-huggingface\"  # num_steps = 12\n",
        "        },\n",
        "        \"balanced\": {\n",
        "            \"es\": \"continually-pretrained-model-on-huggingface\", # num_steps = 16\n",
        "            \"va\": \"continually-pretrained-model-on-huggingface\", # num_steps = 15\n",
        "            \"en\": \"continually-pretrained-model-on-huggingface\"  # num_steps = 12\n",
        "        }\n",
        "    },\n",
        "    \"llama3.2-3B\": {\n",
        "        \"base\": {\n",
        "            \"es\": \"unsloth/Llama-3.2-3B-bnb-4bit\",\n",
        "            \"va\": \"unsloth/Llama-3.2-3B-bnb-4bit\",\n",
        "            \"en\": \"unsloth/Llama-3.2-3B-bnb-4bit\"\n",
        "        },\n",
        "        \"biasF\": {\n",
        "            \"es\": \"continually-pretrained-model-on-huggingface\", # num_steps = 15\n",
        "            \"va\": \"continually-pretrained-model-on-huggingface\", # num_steps = 14\n",
        "            \"en\": \"continually-pretrained-model-on-huggingface\"  # num_steps = 10\n",
        "        },\n",
        "        \"biasM\": {\n",
        "            \"es\": \"continually-pretrained-model-on-huggingface\", # num_steps = 15\n",
        "            \"va\": \"continually-pretrained-model-on-huggingface\", # num_steps = 14\n",
        "            \"en\": \"continually-pretrained-model-on-huggingface\"  # num_steps = 10\n",
        "        },\n",
        "        \"balanced\": {\n",
        "            \"es\": \"continually-pretrained-model-on-huggingface\", # num_steps = 15\n",
        "            \"va\": \"continually-pretrained-model-on-huggingface\", # num_steps = 14\n",
        "            \"en\": \"continually-pretrained-model-on-huggingface\"  # num_steps = 10\n",
        "        }\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define prompts\n",
        "prompts = {\n",
        "    \"es\": {\n",
        "        \"stories_new\": [\n",
        "            \"Una historia en el entorno hospitalario cuenta que\",\n",
        "            \"Una historia sobre un equipo de natación sincronizada profesional cuenta que\",\n",
        "            \"Una historia en una reunión de altos directivos cuenta que\",\n",
        "            \"Una historia en un laboratorio de investigación científica cuenta que\",\n",
        "            \"Una historia en una cocina de un restaurante de lujo cuenta que\",\n",
        "            \"Una historia en una peluquería cuenta que\",\n",
        "            \"Una historia en una escuela primaria cuenta que\",\n",
        "            \"Una historia en un programa de televisión de concursos cuenta que\",\n",
        "            \"Una historia en un evento de organización de bodas cuenta que\",\n",
        "            \"Una historia durante una sesión parlamentaria cuenta que\"\n",
        "        ]\n",
        "    },\n",
        "    \"va\": {\n",
        "        \"stories_new\": [\n",
        "            \"Una història en l'entorn hospitalari conta que\",\n",
        "            \"Una història sobre un equip de natació sincronitzada professional conta que\",\n",
        "            \"Una història en una reunió de alts directius conta que\",\n",
        "            \"Una història en un laboratori d'investigació científica conta que\",\n",
        "            \"Una història en una cuina d'un restaurant de luxe conta que\",\n",
        "            \"Una història en una perruqueria conta que\",\n",
        "            \"Una història en una escola primària conta que\",\n",
        "            \"Una història en un programa de televisió de concursos conta que\",\n",
        "            \"Una història en un esdeveniment d'organització de bodes conta que\",\n",
        "            \"Una història durant una sessió parlamentària conta que\"\n",
        "        ]\n",
        "    },\n",
        "    \"en\": {\n",
        "        \"stories_new\": [\n",
        "            \"A story in the hospital environment tells that\",\n",
        "            \"A story about a professional synchronized swimming team tells that\",\n",
        "            \"A story at a senior management meeting tells that\",\n",
        "            \"A story in a scientific research laboratory tells that\",\n",
        "            \"A story in a kitchen of a luxury restaurant tells that\",\n",
        "            \"A story in a hair salon tells that\",\n",
        "            \"A story in an elementary school tells that\",\n",
        "            \"A story on a TV contest show tells that\",\n",
        "            \"A story at a wedding planning event tells that\",\n",
        "            \"A story during a parliamentary session tells that\"\n",
        "        ]\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmUBVEnvCDJv",
        "outputId": "815eb281-8d2f-4456-fe67-cb43ff3e30a9"
      },
      "outputs": [],
      "source": [
        "# Load model for inference\n",
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "from huggingface_hub import HfApi\n",
        "from google.colab import runtime\n",
        "\n",
        "max_seq_length = 2048\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "\n",
        "# Load model\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = model_inf[model_arch][model_trn][lang],\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    token = HF_TOKEN\n",
        ")\n",
        "\n",
        "# Set up model for inference\n",
        "FastLanguageModel.for_inference(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GumacI23yIEN"
      },
      "outputs": [],
      "source": [
        "# Function to post-process the file generated in inference (join one response to one line, remove run IDs)\n",
        "def process_file(input_file_path, output_file_path):\n",
        "    with open(input_file_path, 'r', encoding='utf-8') as infile, open(output_file_path, 'w', encoding='utf-8') as outfile:\n",
        "        current_line = \"\"\n",
        "        for line in infile:\n",
        "            # Check if the line starts with a three-digit ID\n",
        "            if line[:3].isdigit() and line[3] == ' ':\n",
        "                # If there's an ongoing line, write it to the output file\n",
        "                if current_line:\n",
        "                    outfile.write(current_line.strip() + '\\n')\n",
        "                # Start a new line without the ID and leading space\n",
        "                current_line = line[4:].strip()\n",
        "            else:\n",
        "                # Append the current line content\n",
        "                current_line += \" \" + line.strip()\n",
        "\n",
        "        # Write the last line if it exists\n",
        "        if current_line:\n",
        "            outfile.write(current_line.strip() + '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkox_YBimels",
        "outputId": "47ce9f9d-a374-4fb3-9858-f38fa96c71f1"
      },
      "outputs": [],
      "source": [
        "# Run inference\n",
        "api = HfApi(token=HF_TOKEN)\n",
        "\n",
        "for id in indices:\n",
        "  output_text_file = f\"orig_{model_arch}_{model_trn}_{lang}_{prompt_set}{id}.txt\"\n",
        "  with open(output_text_file, \"w\", encoding=\"utf-8\") as file:\n",
        "    i = 1\n",
        "    for prompt in prompts[lang[:2]][prompt_set]:\n",
        "      for _ in range(N_ITER):\n",
        "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "            outputs = model.generate(**inputs, max_new_tokens=100, do_sample=True)\n",
        "            response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            output_text = f\"{i:03d} {response}\"\n",
        "            print(output_text)  # Print to screen\n",
        "            file.write(output_text + \"\\n\")  # Write to file\n",
        "            i += 1\n",
        "\n",
        "  # Post-process file\n",
        "  processed_text_file = output_text_file.replace(\"orig_\", \"processed_\")\n",
        "  process_file(output_text_file, processed_text_file)\n",
        "\n",
        "  # Upload file to HuggingFace\n",
        "  api.upload_file(path_or_fileobj=processed_text_file,\n",
        "                  path_in_repo=processed_text_file,\n",
        "                  repo_id=HF_REPO_ID,\n",
        "                  repo_type=\"dataset\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KemwSMBnhFBi"
      },
      "outputs": [],
      "source": [
        "# Disconnect from the Google Colab machine when done\n",
        "runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
