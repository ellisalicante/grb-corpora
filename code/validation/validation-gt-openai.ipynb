{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation of the gender representation bias quantification method\n",
    "\n",
    "Validate the LLM-based gender representation bias quantification method on an annotated dataset.\n",
    "\n",
    "Using OpenAI API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "model = \"gpt-4o-2024-11-20\"\n",
    "lang = \"es\"\n",
    "#lang = \"va\"\n",
    "analysis_file_path = \"../../results/validation/\" # output analysis path\n",
    "analysis_file_id = f\"{model}.{lang}.01\" # output analysis file identifier\n",
    "gt_pathname = f\"../../data/validation/gt-{lang}.txt\" # ground truth\n",
    "prompt_pathname = f\"../../data/dataset-analysis/prompt-{lang}.txt\" # prompt pattern\n",
    "examples_pathname = f\"../../data/dataset-analysis/examples-{lang}.txt\" # few-shot examples\n",
    "skiplist_pathname = f\"../../data/dataset-analysis/skiplist-{lang}.txt\" # skip list (words to be ignored)\n",
    "num_sentences = 100 # number of sentences to be analyzed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load various text files\n",
    "def load_text_file(filepath, convert_to_upper=False):\n",
    "    \"\"\"Load text from file.\"\"\"\n",
    "    if filepath is None:\n",
    "        return []\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        if convert_to_upper: # skiplist\n",
    "            return {line.strip().upper() for line in file if line.strip()}\n",
    "        else: # other uses\n",
    "            return file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ground_truth(filepath, skip_words):\n",
    "    \"\"\"\n",
    "    Load and process the ground truth so that:\n",
    "      - The sentence is preserved.\n",
    "      - The answer is uppercased, split by delimiters,\n",
    "        grouped into lists of three (matching the expected format),\n",
    "        and filtered to remove any groups whose first word is in skip_words.\n",
    "    \n",
    "    Returns a DataFrame with columns:\n",
    "      'Sentence' and 'Answer'\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        content = file.read().strip()\n",
    "        blocks = content.split('\\n\\n')\n",
    "        for block in blocks:\n",
    "            sentence, words = block.strip().split(' \"')\n",
    "            sentence = sentence.strip('\"')\n",
    "            # Process words: uppercase and split by delimiters\n",
    "            words = words.strip('\"').upper()\n",
    "            words_list = re.split(r\" - |, |\\n\", words)\n",
    "            grouped = [words_list[i:i+3] for i in range(0, len(words_list), 3)]\n",
    "            # Filter out any group whose first element is in the skip list\n",
    "            filtered_grouped = [group for group in grouped if group and group[0] not in skip_words]\n",
    "            data.append((sentence, filtered_grouped))\n",
    "    return pd.DataFrame(data, columns=['Sentence', 'Answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_response(response_text, skip_words):\n",
    "    \"\"\"\n",
    "    Clean and parse the model's response text into a list of word groups.\n",
    "    The splitting pattern is the same as used for the ground truth.\n",
    "    Groups whose first element appears in skip_words are filtered out.\n",
    "    \"\"\"\n",
    "    cleaned = re.sub(r'\\s+\\n', '\\n', response_text.strip().upper())\n",
    "    words = re.split(r\" - |, |\\n\", cleaned)\n",
    "    groups = [words[i:i+3] for i in range(0, len(words), 3)]\n",
    "    return [group for group in groups if group and group[0] not in skip_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_response(model_words, gt_words):\n",
    "    \"\"\"\n",
    "    Compare the model's response (as a list of word groups) against the ground truth.\n",
    "    \n",
    "    Returns:\n",
    "      identified: correctly identified words (with both attributes correct)\n",
    "      not_identified: words in GT that the model missed\n",
    "      wrongly_analyzed: words where the noun is present but one of the attributes is wrong\n",
    "      wrongly_identified: extra words that do not appear in GT\n",
    "    \"\"\"\n",
    "    identified = []\n",
    "    not_identified = []\n",
    "    wrongly_analyzed = []\n",
    "    # Make a mutable copy for extra words from the model\n",
    "    wrongly_identified = model_words.copy()\n",
    "    \n",
    "    for item in gt_words:\n",
    "        # If an exact match is found, mark as identified\n",
    "        if item in wrongly_identified:\n",
    "            identified.append(item)\n",
    "            wrongly_identified.remove(item)\n",
    "        # If no word with the same noun (first element) is found, it was not identified\n",
    "        elif item[0] not in [w[0] for w in wrongly_identified]:\n",
    "            not_identified.append(item)\n",
    "        # Otherwise, if the noun is present but details differ, mark as wrongly analyzed\n",
    "        elif item[0] in [w[0] for w in wrongly_identified]:\n",
    "            wrongly_analyzed.append(item)\n",
    "            # Remove the first occurrence that matches on the noun\n",
    "            for w in wrongly_identified:\n",
    "                if item[0] == w[0]:\n",
    "                    wrongly_identified.remove(w)\n",
    "                    break\n",
    "                    \n",
    "    return identified, not_identified, wrongly_analyzed, wrongly_identified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed ground truth and the examples text\n",
    "prompt_pattern = load_text_file(prompt_pathname)\n",
    "examples = load_text_file(examples_pathname)\n",
    "skip_words = load_text_file(skiplist_pathname, convert_to_upper=True)\n",
    "df_gt = load_ground_truth(gt_pathname, skip_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process sentences\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "results = []\n",
    "for i in tqdm(range(num_sentences), desc=\"Processing sentences\"):\n",
    "    sentence = df_gt.loc[i, 'Sentence']\n",
    "\n",
    "    # Construct the prompt\n",
    "    prompt = prompt_pattern.replace(\"<EXAMPLES>\", examples).replace(\"<SENTENCE>\", sentence)\n",
    "\n",
    "    # Call the OpenAI API with the provided prompt\n",
    "    response_obj = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    response_text = response_obj.choices[0].message.content\n",
    "    model_words = parse_response(response_text, skip_words)\n",
    "    \n",
    "    # Compare the model response with the processed ground truth\n",
    "    gt_words = df_gt.loc[i, 'Answer']\n",
    "    identified, not_identified, wrongly_analyzed, wrongly_identified = analyze_response(model_words, gt_words)\n",
    "    \n",
    "    # Append the evaluation result for this sentence\n",
    "    results.append({\n",
    "        'ID': i + 1,\n",
    "        'Sentence': sentence,\n",
    "        'Ground Truth': gt_words,\n",
    "        'Response': model_words,\n",
    "        'Num Identified': len(identified),\n",
    "        'Num Not Identified': len(not_identified),\n",
    "        'Num Incorrectly Analyzed': len(wrongly_analyzed),\n",
    "        'Num Incorrectly Identified': len(wrongly_identified),\n",
    "        'Identified': identified,\n",
    "        'Not Identified': not_identified,\n",
    "        'Incorrectly Analyzed': wrongly_analyzed,\n",
    "        'Incorrectly Identified': wrongly_identified\n",
    "    })\n",
    "\n",
    "# Create a DataFrame from the results and write to the Excel file\n",
    "df_results = pd.DataFrame(results)\n",
    "output_file = os.path.join(analysis_file_path, f'validation-{analysis_file_id}.xlsx')\n",
    "\n",
    "# Check if the output file exists\n",
    "if os.path.exists(output_file):\n",
    "    # Add a timestamp to the filename\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    new_output_file = f\"{os.path.splitext(output_file)[0]}-{timestamp}{os.path.splitext(output_file)[1]}\"\n",
    "    warnings.warn(f\"Output file already exists, renaming with timestamp.\")\n",
    "    output_file = new_output_file\n",
    "\n",
    "# Save output to Excel and append analysis cells\n",
    "os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "    df_results.to_excel(writer, index=False, sheet_name='Sheet1')\n",
    "    worksheet = writer.sheets['Sheet1']\n",
    "    \n",
    "    # Calculate row indices\n",
    "    last_data_row = num_sentences + 1\n",
    "    total_row = last_data_row + 1\n",
    "    analysis_row_start = total_row + 3\n",
    "\n",
    "    # Sums for each column\n",
    "    worksheet.cell(row=total_row, column=1, value=\"Total\")\n",
    "    worksheet.cell(row=total_row, column=5, value=f\"=SUM(E2:E{last_data_row})\")\n",
    "    worksheet.cell(row=total_row, column=6, value=f\"=SUM(F2:F{last_data_row})\")\n",
    "    worksheet.cell(row=total_row, column=7, value=f\"=SUM(G2:G{last_data_row})\")\n",
    "    worksheet.cell(row=total_row, column=8, value=f\"=SUM(H2:H{last_data_row})\")\n",
    "    \n",
    "    # Accuracy %, Precision, Recall, and F1 Score\n",
    "    # Captions\n",
    "    worksheet.cell(row=analysis_row_start, column=1, value=\"Accuracy %\")\n",
    "    worksheet.cell(row=analysis_row_start + 1, column=1, value=\"Precision\")\n",
    "    worksheet.cell(row=analysis_row_start + 2, column=1, value=\"Recall\")\n",
    "    worksheet.cell(row=analysis_row_start + 3, column=1, value=\"F1 Score\")    \n",
    "    # Formulas\n",
    "    worksheet.cell(row=analysis_row_start, column=2,\n",
    "                   value=f\"=E{total_row}/(E{total_row}+F{total_row}+G{total_row})*100\")\n",
    "    worksheet.cell(row=analysis_row_start + 1, column=2,\n",
    "                   value=f\"=E{total_row}/(G{total_row}+H{total_row}+E{total_row})\")\n",
    "    worksheet.cell(row=analysis_row_start + 2, column=2,\n",
    "                   value=f\"=E{total_row}/(E{total_row}+F{total_row})\")\n",
    "    worksheet.cell(row=analysis_row_start + 3, column=2,\n",
    "                   value=f\"=2*B{analysis_row_start + 1}*B{analysis_row_start + 2}/(B{analysis_row_start + 1}+B{analysis_row_start + 2})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
